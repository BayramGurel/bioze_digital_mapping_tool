{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "from shapely.geometry import shape\n",
    "import fiona\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls(base_url):\n",
    "    \"\"\"\n",
    "    Scrapes all URLs from a webpage.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of URLs found on the webpage.\n",
    "    \"\"\"\n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    # If the GET request is successful, the status code will be 200\n",
    "    if response.status_code == 200:\n",
    "        # Get the content of the response\n",
    "        page_content = response.content\n",
    "\n",
    "        # Create a BeautifulSoup object and specify the parser\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "        # Find all the anchor tags in the HTML\n",
    "        # Extract the href attribute and add it to a list\n",
    "        urls = [a['href'] for a in soup.find_all('a', href=True) if 'cbs_vk100' in a['href']]\n",
    "\n",
    "        return urls\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to scrape {base_url}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, year, download_path, extract_path):\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from a URL and extracts its contents.\n",
    "\n",
    "    Args:\n",
    "        year (int): The year of the data.\n",
    "        download_path (str): Path to save the downloaded ZIP file.\n",
    "        extract_path (str): Path to extract the contents of the ZIP file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file was successfully downloaded and extracted, False otherwise.\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception if the request was unsuccessful\n",
    "    \t\n",
    "        # Save the file\n",
    "        with open(download_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Extract the file\n",
    "        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        \n",
    "        return True  # The file was successfully downloaded and extracted\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading or extracting file: {e}\")\n",
    "        return False  # An error occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geojson(url_gpkg, resolution, output_filepath):\n",
    "  \"\"\"\n",
    "  ... (function docstring)\n",
    "  \"\"\"\n",
    "  try:\n",
    "    if not os.path.exists(url_gpkg):\n",
    "      raise FileNotFoundError(f\"Input file not found: {url_gpkg}\")\n",
    "\n",
    "    with fiona.open(url_gpkg, 'r') as src:\n",
    "      crs = src.crs\n",
    "      population_data = gpd.GeoDataFrame()\n",
    "\n",
    "      for feat in src:\n",
    "        # Process each feature individually\n",
    "        df = gpd.GeoDataFrame([feat], crs=crs)\n",
    "        # Select columns and explode\n",
    "        selected_data = df[['properties', 'geometry']]\n",
    "        selected_data['aantal_inwoners'] = selected_data['properties'].apply(lambda x: x['aantal_inwoners'])\n",
    "        selected_data.drop(columns=['properties'], inplace=True)\n",
    "        centroid_points = selected_data.explode(index_parts=True)\n",
    "        # Handle missing values (replace with 0 for this example)\n",
    "        centroid_points.fillna(0, inplace=True)\n",
    "        # Set CRS\n",
    "        centroid_points = centroid_points.set_crs(4326, allow_override=True)\n",
    "\n",
    "        # Aggregate to H3 cells (consider vectorization for efficiency)\n",
    "        h3_cells = h3.geo_to_h3(centroid_points['geometry'].tolist())\n",
    "\n",
    "        # Aggregate to H3 cells (consider vectorization for efficiency)\n",
    "        h3_cells = []\n",
    "        for geom in centroid_points['geometry'].tolist():\n",
    "            lat, lng = geom.y, geom.x  # Extract latitude and longitude\n",
    "            h3_cell = h3.geo_to_h3(lat, lng, resolution)  # Convert to H3 cell\n",
    "            h3_cells.append(h3_cell)\n",
    "\n",
    "        # Append H3 data to population_data\n",
    "        population_data = population_data.append(pd.DataFrame(h3_cells, columns=['h3']), ignore_index=True)\n",
    "\n",
    "\n",
    "    # Save H3 data to CSV after processing all features\n",
    "    population_data.to_csv(output_filepath)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Get the current year\n",
    "    current_year = datetime.datetime.now().year -1\n",
    "\n",
    "    # Scrape the URLs from the website\n",
    "    base_url = 'https://www.cbs.nl/nl-nl/dossier/nederland-regionaal/geografische-data/kaart-van-100-meter-bij-100-meter-met-statistieken#:~:text=In%20deze%20kaart%20met%20vierkanten,en%20nabijheid%20van%20voorzieningen%20samengesteld.'\n",
    "    urls = scrape_urls(base_url)\n",
    "    # print(urls)\n",
    "\n",
    "    # Try downloading the file for the current year and previous years\n",
    "    for year in range(current_year - 1, current_year - 12, -1):  # Try for the last 10 years\n",
    "        # Construct the file paths using the year\n",
    "        download_path = f'./cbs_{year}.zip'\n",
    "        extract_path = f'./cbs_{year}'\n",
    "        url_gpkg = os.path.join(extract_path, f'cbs_vk100_{year}_v1.gpkg')\n",
    "        output_filepath = f'./cbs_{year}_h3.csv'\n",
    "\n",
    "        # Find the URL for the current year\n",
    "        url = next((u for u in urls if str(year) in u), None)\n",
    "\n",
    "        if url:\n",
    "            # Download and extract the file\n",
    "            if download_and_extract(url, year, download_path, extract_path):\n",
    "\n",
    "                # If successful, process the data and break the loop\n",
    "                process_geojson(url_gpkg, 9, output_filepath)\n",
    "                break\n",
    "        else:\n",
    "            print(f\"No URL available for {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data: _API_FUNCTIONS.geo_to_h3() missing 2 required positional arguments: 'lng' and 'resolution'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
